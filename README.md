ğŸ² Missing Data Handling via Random Sample Imputation & Missing Indicators
Preserving Data Distribution While Filling Gaps

A smart, distribution-aware approach to imputing missing values that maintains the original variance, shape, and statistical integrity of your features â€” far beyond simple mean or median filling.

âœ… Ideal for datasets where preserving real-world variability is critical for unbiased modeling.

ğŸ“Œ Overview
This notebook explores Random Sample Imputation, a powerful technique that replaces missing entries by drawing values at random from the observed data of the same feature. Unlike central-tendency methods, this approach keeps the original distribution intact â€” making it especially valuable for numerical and categorical features alike.

âœ¨ Why It Stands Out

Maintains variance and covariance structure
Preserves distribution shape (confirmed via visual density plots)
Works seamlessly for both numerical and categorical data
Avoids the artificial â€œspikeâ€ at the mean/median that distorts model learning
ğŸ” Key Workflow Highlights

ğŸ“Š Distribution Preservation
Instead of collapsing missing values to a single point (like the mean), random sampling pulls from the actual pool of existing values. This ensures the post-imputation feature closely mirrors the original in both spread and shape â€” validated through statistical summaries and kernel density estimates.

ğŸ”¤ Categorical Consistency
The same logic applies to text-based features like Category or Stock: missing labels are replaced by randomly selected existing categories, preserving their natural proportions in the dataset. This prevents skewing class balances during preprocessing.

ğŸ“ˆ Impact Assessment
The notebook includes side-by-side comparisons showing:

How mean/median imputation flattens variance and distorts distributions
How random sampling retains the original statistical profile
Why this leads to more reliable and generalizable models
ğŸ’¡ Key Takeaways for Data Scientists

ğŸ§ª Imputation is transformation: Every method changes your data â€” always validate its impact
ğŸ“ Prefer median over mean for central-tendency imputation (robust to outliers)
ğŸ¯ Random Sample Imputation shines when data is Missing At Random (MAR) â€” itâ€™s simple, effective, and statistically sound
ğŸ”’ Prevent data leakage: Always derive your imputation strategy (including the pool of values to sample from) only from the training set, then apply it consistently to test data
ğŸ› ï¸ Use pipelines: Scikit-learnâ€™s fit/transform pattern enforces best practices automatically
ğŸš€ When to Use This Method

Your feature has meaningful variance you donâ€™t want to lose
Youâ€™re working with mixed data types (numeric + categorical)
You need a general-purpose, model-agnostic imputation strategy
Downstream models are sensitive to distributional shifts (e.g., linear models, clustering)
ğŸ“¬ Feedback & Contributions
Want to compare with KNN imputation, add missing indicator flags, or test on new datasets?
ğŸ‘‰ Open an issue or submit a pull request â€” all contributions are welcome! ğŸ¤
